{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以Word Embedding取得同義詞\n",
    "\n",
    "### 詞向量訓練教學\n",
    "- https://github.com/zake7749/word2vec-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import jieba\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞\n",
    "- jieba\n",
    "- pynlpir(未用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segWords():\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "\n",
    "    # load stopwords set\n",
    "    stopword_set = set()\n",
    "    with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "            for stopword in stopwords:\n",
    "                stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "    output = open('case_seg.txt', 'w', encoding='utf-8')\n",
    "    with open('case.csv', 'r', encoding='utf-8') as content :\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False)\n",
    "            for word in words:\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word + ' ')\n",
    "            output.write('\\n')\n",
    "\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                logging.info(\"已完成前 %d 行的斷詞\" % (texts_num + 1))\n",
    "    output.close()\n",
    "    \n",
    "segWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-04 13:18:55,949 : INFO : collecting all words and their counts\n",
      "2018-12-04 13:18:55,953 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-04 13:18:56,404 : INFO : collected 86468 word types from a corpus of 1689683 raw words and 5429 sentences\n",
      "2018-12-04 13:18:56,405 : INFO : Loading a fresh vocabulary\n",
      "2018-12-04 13:18:56,459 : INFO : min_count=5 retains 19894 unique words (23% of original 86468, drops 66574)\n",
      "2018-12-04 13:18:56,459 : INFO : min_count=5 leaves 1587645 word corpus (93% of original 1689683, drops 102038)\n",
      "2018-12-04 13:18:56,515 : INFO : deleting the raw counts dictionary of 86468 items\n",
      "2018-12-04 13:18:56,518 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2018-12-04 13:18:56,519 : INFO : downsampling leaves estimated 1490826 word corpus (93.9% of prior 1587645)\n",
      "2018-12-04 13:18:56,570 : INFO : estimated required memory for 19894 words and 300 dimensions: 57692600 bytes\n",
      "2018-12-04 13:18:56,571 : INFO : resetting layer weights\n",
      "2018-12-04 13:18:56,814 : INFO : training model with 3 workers on 19894 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=30\n",
      "2018-12-04 13:18:57,826 : INFO : EPOCH 1 - PROGRESS: at 28.97% examples, 469453 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:18:58,829 : INFO : EPOCH 1 - PROGRESS: at 72.57% examples, 482370 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:18:59,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 13:18:59,837 : INFO : EPOCH 1 - PROGRESS: at 99.67% examples, 490492 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-04 13:18:59,838 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 13:18:59,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 13:18:59,840 : INFO : EPOCH - 1 : training on 1689683 raw words (1490703 effective words) took 3.0s, 492868 effective words/s\n",
      "2018-12-04 13:19:00,867 : INFO : EPOCH 2 - PROGRESS: at 30.72% examples, 479973 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:01,881 : INFO : EPOCH 2 - PROGRESS: at 75.43% examples, 499360 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:02,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 13:19:02,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 13:19:02,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 13:19:02,816 : INFO : EPOCH - 2 : training on 1689683 raw words (1490963 effective words) took 3.0s, 501356 effective words/s\n",
      "2018-12-04 13:19:03,838 : INFO : EPOCH 3 - PROGRESS: at 30.72% examples, 482854 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:04,861 : INFO : EPOCH 3 - PROGRESS: at 74.95% examples, 494968 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:05,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 13:19:05,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 13:19:05,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 13:19:05,803 : INFO : EPOCH - 3 : training on 1689683 raw words (1490552 effective words) took 3.0s, 499514 effective words/s\n",
      "2018-12-04 13:19:06,809 : INFO : EPOCH 4 - PROGRESS: at 29.97% examples, 482354 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:07,821 : INFO : EPOCH 4 - PROGRESS: at 73.88% examples, 494220 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:08,852 : INFO : EPOCH 4 - PROGRESS: at 97.16% examples, 464646 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:08,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 13:19:08,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 13:19:09,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 13:19:09,033 : INFO : EPOCH - 4 : training on 1689683 raw words (1490738 effective words) took 3.2s, 462203 effective words/s\n",
      "2018-12-04 13:19:10,035 : INFO : EPOCH 5 - PROGRESS: at 26.36% examples, 430623 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:11,043 : INFO : EPOCH 5 - PROGRESS: at 69.26% examples, 461649 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:12,052 : INFO : EPOCH 5 - PROGRESS: at 95.80% examples, 457162 words/s, in_qsize 5, out_qsize 0\n",
      "2018-12-04 13:19:12,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-04 13:19:12,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-04 13:19:12,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-04 13:19:12,317 : INFO : EPOCH - 5 : training on 1689683 raw words (1490673 effective words) took 3.3s, 454121 effective words/s\n",
      "2018-12-04 13:19:12,319 : INFO : training on a 8448415 raw words (7453629 effective words) took 15.5s, 480755 effective words/s\n",
      "2018-12-04 13:19:12,320 : INFO : saving Word2Vec object under case_word2vec_w30.model, separately None\n",
      "2018-12-04 13:19:12,322 : INFO : not storing attribute vectors_norm\n",
      "2018-12-04 13:19:12,326 : INFO : not storing attribute cum_table\n",
      "2018-12-04 13:19:12,783 : INFO : saved case_word2vec_w30.model\n"
     ]
    }
   ],
   "source": [
    "def trainModel():\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    sentences = word2vec.LineSentence(\"case_seg.txt\")\n",
    "    model = word2vec.Word2Vec(sentences, size=300, window=30)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    model.save(\"case_word2vec_w30.model\")\n",
    "\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\")\n",
    "\n",
    "trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-04 13:19:45,803 : INFO : loading Word2Vec object from case_word2vec_w30.model\n",
      "2018-12-04 13:19:46,165 : INFO : loading wv recursively from case_word2vec_w30.model.wv.* with mmap=None\n",
      "2018-12-04 13:19:46,166 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-12-04 13:19:46,166 : INFO : loading vocabulary recursively from case_word2vec_w30.model.vocabulary.* with mmap=None\n",
      "2018-12-04 13:19:46,167 : INFO : loading trainables recursively from case_word2vec_w30.model.trainables.* with mmap=None\n",
      "2018-12-04 13:19:46,167 : INFO : setting ignored attribute cum_table to None\n",
      "2018-12-04 13:19:46,168 : INFO : loaded case_word2vec_w30.model\n"
     ]
    }
   ],
   "source": [
    "#模型讀取方式\n",
    "model = word2vec.Word2Vec.load(\"case_word2vec_w30.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '披衣' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1e06f0e725e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'披衣'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim-3.4.0-py3.6-macosx-10.7-x86_64.egg/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim-3.4.0-py3.6-macosx-10.7-x86_64.egg/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \"\"\"\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim-3.4.0-py3.6-macosx-10.7-x86_64.egg/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim-3.4.0-py3.6-macosx-10.7-x86_64.egg/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim-3.4.0-py3.6-macosx-10.7-x86_64.egg/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '披衣' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model['披衣']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2018-12-04 13:19:54,411 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似詞前30排序\n",
      "高热,0.8824350237846375\n",
      "咽痛,0.836197018623352\n",
      "流涕,0.8288683891296387\n",
      "痒,0.8238584399223328\n",
      "瘆,0.8153531551361084\n",
      "出疹,0.8119768500328064\n",
      "鼻塞,0.8097071051597595\n",
      "寒战,0.8093984127044678\n",
      "身热,0.8005375266075134\n",
      "恶寒,0.7970983982086182\n",
      "微咳,0.7918537259101868\n",
      "炎夏,0.7888762354850769\n",
      "清涕,0.7814413905143738\n",
      "无汗,0.7807796001434326\n",
      "低热,0.7784580588340759\n",
      "39.5,0.7764464616775513\n",
      "38,0.7702428698539734\n",
      "未退,0.764227032661438\n",
      "疹,0.7634611129760742\n",
      "C,0.7626867890357971\n",
      "鼻流,0.7621762752532959\n",
      "痰色,0.7615824341773987\n",
      "发热时,0.7587440013885498\n",
      "°,0.7579063773155212\n",
      "热退,0.7573127746582031\n",
      "青霉素,0.7565338015556335\n",
      "体温,0.7557786107063293\n",
      "秋燥,0.7493321895599365\n",
      "高烧,0.7490290999412537\n",
      "喷嚏,0.7443285584449768\n"
     ]
    }
   ],
   "source": [
    "print(\"相似詞前30排序\")\n",
    "query_term = \"发热\"\n",
    "res = model.most_similar(query_term,topn = 30)\n",
    "for item in res:\n",
    "    print(item[0]+\",\"+str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
